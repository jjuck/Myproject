{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 448\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 20\n",
    "LATENT_DIM = 10\n",
    "\n",
    "image_data_dir = 'DATA'\n",
    "image_datagen = image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0,\n",
    "    height_shift_range=0,\n",
    "    zoom_range=0,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.9, 1.1]\n",
    ")\n",
    "\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    image_data_dir,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='input'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vae\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"vae\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>), │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,377,866</span> │\n",
       "│                                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)]             │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,164,867</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_30 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ encoder (\u001b[38;5;33mFunctional\u001b[0m)            │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m), │     \u001b[38;5;34m2,377,866\u001b[0m │\n",
       "│                                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)]             │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder (\u001b[38;5;33mFunctional\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │     \u001b[38;5;34m2,164,867\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,542,733</span> (17.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,542,733\u001b[0m (17.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,542,733</span> (17.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,542,733\u001b[0m (17.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tried to convert 'x' to a tensor and failed. Error: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m vae\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# 학습\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    115\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[16], line 102\u001b[0m, in \u001b[0;36mvae_loss\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvae_loss\u001b[39m(inputs, outputs):\n\u001b[1;32m--> 102\u001b[0m     kl_loss \u001b[38;5;241m=\u001b[39m \u001b[43mkl_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_log_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     reconstruction_loss \u001b[38;5;241m=\u001b[39m reconstruction_loss_fn(inputs, outputs)\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reconstruction_loss \u001b[38;5;241m+\u001b[39m kl_loss\n",
      "Cell \u001b[1;32mIn[16], line 78\u001b[0m, in \u001b[0;36mkl_loss_fn\u001b[1;34m(z_mean, z_log_var)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkl_loss_fn\u001b[39m(z_mean, z_log_var):\n\u001b[0;32m     77\u001b[0m     kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(\n\u001b[1;32m---> 78\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m z_log_var \u001b[38;5;241m-\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_mean\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39mexp(z_log_var),\n\u001b[0;32m     79\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     80\u001b[0m     )\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_mean(kl_loss)\n",
      "\u001b[1;31mValueError\u001b[0m: Tried to convert 'x' to a tensor and failed. Error: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "class VAESampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.keras.backend.shape(z_mean)[0]\n",
    "        z_log_var = tf.clip_by_value(z_log_var, -20, 2)\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, LATENT_DIM))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def build_vae_encoder(input_shape, latent_dim):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer=HeNormal())(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer=HeNormal())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer=HeNormal())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean', \n",
    "                         kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.1))(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var',\n",
    "                            kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.1))(x)\n",
    "\n",
    "    z = VAESampling()([z_mean, z_log_var])\n",
    "    \n",
    "    return tf.keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "def build_vae_decoder(latent_dim, original_shape):\n",
    "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "\n",
    "    x = layers.Dense(256, activation='relu')(latent_inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = layers.Dense((original_shape[0] // 8) * (original_shape[1] // 8) * 128, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Reshape((original_shape[0] // 8, original_shape[1] // 8, 128))(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same',\n",
    "                             kernel_initializer=HeNormal())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same',\n",
    "                             kernel_initializer=HeNormal())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same',\n",
    "                             kernel_initializer=HeNormal())(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "    outputs = layers.Conv2DTranspose(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    return tf.keras.Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, latent_dim, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.latent_dim = latent_dim\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    MSE(data, reconstruction),\n",
    "                    axis=[1, 2]\n",
    "                )\n",
    "            ) * IMAGE_SIZE\n",
    "\n",
    "            z_log_var = tf.clip_by_value(z_log_var, -20, 2)\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var),\n",
    "                    axis=1\n",
    "                )\n",
    "            )\n",
    "\n",
    "            total_loss = reconstruction_loss + 0.001 * kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, 1.0)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result()\n",
    "        }\n",
    "\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "vae_encoder = build_vae_encoder(input_shape, LATENT_DIM)\n",
    "vae_decoder = build_vae_decoder(LATENT_DIM, input_shape)\n",
    "\n",
    "vae = VAE(vae_encoder, vae_decoder, LATENT_DIM)\n",
    "vae.compile(optimizer=Adam(learning_rate=0.0001))\n",
    "\n",
    "history = vae.fit(\n",
    "    image_generator,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualize(vae, input_images, num_samples=5):\n",
    "    input_subset = input_images[:num_samples]\n",
    "\n",
    "    z_mean, z_log_var, z = vae.encoder(input_subset)\n",
    "    print(z)\n",
    "\n",
    "    reconstructed_images = vae.decoder(z)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(2, num_samples, i + 1)\n",
    "        plt.imshow(input_subset[i])\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title('Original Images')\n",
    "\n",
    "        plt.subplot(2, num_samples, num_samples + i + 1)\n",
    "        plt.imshow(reconstructed_images[i])\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title('Reconstructed Images')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "images, _ = next(image_generator)\n",
    "\n",
    "predict_and_visualize(vae, images, num_samples=2)\n",
    "\n",
    "def visualize_multiple_batches(vae, generator, numbatches=3):\n",
    "    for _ in range(numbatches):\n",
    "        images, _ = next(generator)\n",
    "        predict_and_visualize(vae, images, num_samples=2)\n",
    "        plt.show()\n",
    "\n",
    "visualize_multiple_batches(vae, image_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reconstruct_image_from_path(vae, image_path):\n",
    "    \"\"\"\n",
    "    이미지 파일 경로를 입력받아 VAE로 재구성된 이미지를 출력합니다.\n",
    "    \n",
    "    Parameters:\n",
    "        vae: 훈련된 VAE 모델\n",
    "        image_path: 이미지 파일 경로 (str)\n",
    "    \"\"\"\n",
    "    # 이미지 로드 및 전처리\n",
    "    img = image.load_img(image_path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = img_array / 255.0  # 정규화\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # 배치 차원 추가\n",
    "    \n",
    "    # VAE로 이미지 재구성\n",
    "    z_mean, z_log_var, z = vae.encoder(img_array)\n",
    "    reconstructed = vae.decoder(z)\n",
    "    \n",
    "    # 결과 시각화\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # 원본 이미지\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_array[0])\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 재구성된 이미지\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(reconstructed[0])\n",
    "    plt.title('Reconstructed Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "image_path = \"path/to/your/image.jpg\"\n",
    "reconstruct_image_from_path(vae, image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
