{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ssim() got an unexpected keyword argument 'channel_axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 135\u001b[0m\n\u001b[0;32m    133\u001b[0m         reconstructed_images \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images)):\n\u001b[1;32m--> 135\u001b[0m             mse, psnr, ssim_value, perceptual_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_reconstruction_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreconstructed_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m             test_errors\u001b[38;5;241m.\u001b[39mappend([mse, psnr, ssim_value, perceptual_loss])\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# 평균 오류 계산\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 89\u001b[0m, in \u001b[0;36mcalculate_reconstruction_error\u001b[1;34m(original_image, reconstructed_image)\u001b[0m\n\u001b[0;32m     86\u001b[0m original_image_np \u001b[38;5;241m=\u001b[39m (original_image_np \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     87\u001b[0m reconstructed_image_np \u001b[38;5;241m=\u001b[39m (reconstructed_image_np \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m ssim_value \u001b[38;5;241m=\u001b[39m \u001b[43mssim\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_image_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreconstructed_image_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_range\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m#LPIPS\u001b[39;00m\n\u001b[0;32m     92\u001b[0m perceptual_loss_fn \u001b[38;5;241m=\u001b[39m lpips\u001b[38;5;241m.\u001b[39mLPIPS(net\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvgg\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# VGG 기반 Perceptual Loss\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ssim() got an unexpected keyword argument 'channel_axis'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from diffusers import UNet2DModel\n",
    "from timm.models import resnet18\n",
    "import lpips\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 설정\n",
    "class Config:\n",
    "    image_size = 256\n",
    "    train_batch_size = 32\n",
    "    test_batch_size = 32\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-4\n",
    "    train_ratio = 0.8 # 학습 데이터 비율\n",
    "    data_dir = \"DATA\" # 데이터 경로\n",
    "\n",
    "config = Config()\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(*list(backbone.children())[:-2])  # ResNet18에서 마지막 두 레이어 제거\n",
    "        self.channel_reduction = nn.Conv2d(512, 3, kernel_size=1)  # 채널 축소 (512 → 3)\n",
    "        self.decoder = UNet2DModel(\n",
    "            sample_size=256,  # 입력 이미지 크기\n",
    "            in_channels=3,\n",
    "            out_channels=3,\n",
    "            layers_per_block=2,\n",
    "            block_out_channels=(128, 256, 512),\n",
    "            down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\"),\n",
    "            up_block_types=(\"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        latent = self.channel_reduction(latent)  # 채널 수 축소 (512 → 3)\n",
    "        batch_size = latent.shape[0]\n",
    "        timestep = torch.zeros(batch_size, dtype=torch.long, device=latent.device)  # timestep 설정\n",
    "        decoded = self.decoder(latent, timestep).sample  # U-Net 디코더\n",
    "        \n",
    "        # 출력 크기를 원본 크기로 보정\n",
    "        decoded = F.interpolate(decoded, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        return decoded\n",
    "\n",
    "# 모델 및 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "backbone = resnet18(pretrained=True)\n",
    "model = Autoencoder(backbone).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 손실 함수 및 오류 계산 함수 (이전 코드와 동일)\n",
    "def ssim(X, Y, data_range):\n",
    "    C1 = (0.01 * data_range)**2\n",
    "    C2 = (0.03 * data_range)**2\n",
    "\n",
    "    mu_x = nn.AvgPool2d(kernel_size=3, stride=1)(X)\n",
    "    mu_y = nn.AvgPool2d(kernel_size=3, stride=1)(Y)\n",
    "\n",
    "    sigma_x = nn.AvgPool2d(kernel_size=3, stride=1)(X**2) - mu_x**2\n",
    "    sigma_y = nn.AvgPool2d(kernel_size=3, stride=1)(Y**2) - mu_y**2\n",
    "    sigma_xy = nn.AvgPool2d(kernel_size=3, stride=1)(X*Y) - mu_x*mu_y\n",
    "\n",
    "    SSIM = (2*mu_x*mu_y + C1)*(2*sigma_xy + C2) / (mu_x**2 + mu_y**2 + C1)*(sigma_x + sigma_y + C2)\n",
    "    return SSIM.mean()\n",
    "\n",
    "def calculate_reconstruction_error(original_image, reconstructed_image):\n",
    "    # MSE (Pixel-wise 차이)\n",
    "    mse = nn.MSELoss()(original_image, reconstructed_image).item()\n",
    "\n",
    "    # PSNR (Peak Signal-to-Noise Ratio)\n",
    "    mse_np = mse # tensor to np\n",
    "    psnr = 10 * np.log10(1 / mse_np) if mse_np > 0 else float('inf')\n",
    "    \n",
    "    # SSIM (Structural Similarity)\n",
    "    original_image_np = original_image.cpu().detach().numpy()[0].transpose(1,2,0)\n",
    "    reconstructed_image_np = reconstructed_image.cpu().detach().numpy()[0].transpose(1,2,0)\n",
    "    original_image_np = (original_image_np * 0.5 + 0.5)\n",
    "    reconstructed_image_np = (reconstructed_image_np * 0.5 + 0.5)\n",
    "\n",
    "    ssim_value = ssim(original_image_np, reconstructed_image_np, channel_axis=2, data_range = 1)\n",
    "    \n",
    "    #LPIPS\n",
    "    perceptual_loss_fn = lpips.LPIPS(net='vgg').to(\"cuda\") # VGG 기반 Perceptual Loss\n",
    "    perceptual_loss = perceptual_loss_fn(original_image, reconstructed_image).item()\n",
    "    \n",
    "    return mse, psnr, ssim_value, perceptual_loss\n",
    "\n",
    "# 데이터 로더\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((config.image_size, config.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=config.data_dir, transform=transform)\n",
    "\n",
    "# 학습/테스트 데이터 분할\n",
    "train_size = int(config.train_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=True, num_workers=os.cpu_count())\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config.test_batch_size, shuffle=False, num_workers=os.cpu_count())\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(config.num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        images, _ = batch # _는 클래스 레이블 (사용하지 않음)\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed_images = model(images)\n",
    "        loss = loss_fn(reconstructed_images, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 검증 (테스트 데이터셋의 일부 사용)\n",
    "    model.eval()\n",
    "    test_errors = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            images, _ = batch\n",
    "            images = images.to(device)\n",
    "            reconstructed_images = model(images)\n",
    "            for i in range(len(images)):\n",
    "                mse, psnr, ssim_value, perceptual_loss = calculate_reconstruction_error(images[i].unsqueeze(0), reconstructed_images[i].unsqueeze(0))\n",
    "                test_errors.append([mse, psnr, ssim_value, perceptual_loss])\n",
    "\n",
    "    # 평균 오류 계산\n",
    "    test_errors = np.array(test_errors)\n",
    "    avg_mse, avg_psnr, avg_ssim, avg_lpips = np.mean(test_errors, axis=0)\n",
    "    print(f\"Epoch {epoch+1}, Test MSE: {avg_mse}, PSNR: {avg_psnr}, SSIM: {avg_ssim}, LPIPS: {avg_lpips}\")\n",
    "\n",
    "# 임계값 설정 (테스트 데이터셋 전체 사용)\n",
    "model.eval()\n",
    "all_test_errors = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        images, _ = batch\n",
    "        images = images.to(device)\n",
    "        reconstructed_images = model(images)\n",
    "        for i in range(len(images)):\n",
    "            mse, psnr, ssim_value, perceptual_loss = calculate_reconstruction_error(images[i].unsqueeze(0), reconstructed_images[i].unsqueeze(0))\n",
    "            all_test_errors.append([mse, psnr, ssim_value, perceptual_loss])\n",
    "\n",
    "all_test_errors = np.array(all_test_errors)\n",
    "\n",
    "# 임계값 설정 (예시: MSE 기준)\n",
    "mse_threshold = np.percentile(all_test_errors[:, 0], 95) # 상위 5%를 임계값으로 설정 (조정 가능)\n",
    "print(f\"MSE Threshold: {mse_threshold}\")\n",
    "\n",
    "# 새로운 이미지에 대한 분류 (추론)\n",
    "def classify_image(image_path, threshold=mse_threshold):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        reconstructed_image = model(image)\n",
    "        mse, psnr, ssim_value, perceptual_loss = calculate_reconstruction_error(image, reconstructed_image)\n",
    "    if mse > threshold:\n",
    "        return \"Unseen Class\", mse, psnr, ssim_value, perceptual_loss\n",
    "    else:\n",
    "        return \"Seen Class\", mse, psnr, ssim_value, perceptual_loss\n",
    "\n",
    "# 새로운 이미지 테스트\n",
    "test_image_path = \"160_master.jpeg\" # 테스트할 이미지 경로\n",
    "classification_result, mse, psnr, ssim_value, perceptual_loss = classify_image(test_image_path)\n",
    "print(f\"Image Classification: {classification_result}, MSE: {mse}, PSNR: {psnr}, SSIM: {ssim_value}, LPIPS: {perceptual_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
